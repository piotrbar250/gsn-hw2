{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZN1tZ4BoHFKn"
      },
      "source": [
        "# **Homework 2: Image segmentation without user input**\n",
        "\n",
        "## Overview\n",
        "This homework has two parts.\n",
        "\n",
        "The first part of this homework is to implement [Grad-CAM](https://arxiv.org/pdf/1610.02391) â€“ a method for producing saliency maps (heatmaps of regions that are most relevant to a model) that uses both activations and gradients of the feature maps of a convolutional layer. Feature maps of deeper convolutional layers represent more high-level features, while preserving rough spatial structure, which makes them a good candidate for explaining a model's output.\n",
        "\n",
        "The second part of this homework is to use SAM for image segmentation without user input.\n",
        "SAM [(Segment Anything Model v1)](https://arxiv.org/pdf/2304.02643) is a popular family of open-weight models for image segmentation (based on the vision transformer ViT and CLIP).\n",
        "The model takes as input an image to be segmented and additionaly bounding boxes, point coordinates etc. clarifying the object of interest to be segmented. It can output many proposed segmentations of many objects on one image. Your task will be to find appropriate point coordinates automatically, so that SAM can be used with just an image input to segment particular objects.\n",
        "\n",
        "---\n",
        "\n",
        "## Dataset\n",
        "\n",
        "You will use a small custom dataset based on [CIFAR10](https://en.wikipedia.org/wiki/CIFAR-10), but containing images with one of five objects (circle, square, diamond, triangle, star), with ground-truth segmentations of that object.\n",
        "\n",
        "---\n",
        "\n",
        "## GradCAM task\n",
        "\n",
        "For the first task:\n",
        "* Read the original [Grad-CAM](https://arxiv.org/pdf/1610.02391) paper.\n",
        "* Implement it, without using non-standard packages (the only imports allowed are built-ins, torch, torchvision, numpy, scipy, and helpers like cv2, PIL, tqdm, matplotlib).\n",
        "* The result should be a class named `GradCAM` with methods:\n",
        "    * `def __init__(model: nn.Module, target_layers=Iterable[nn.Module])`\n",
        "    * `def __call__(self, image: Tensor, targets: Iterable[int] | None = None) -> np.ndarray` where\n",
        "        * `image` is an input to `model` (a normalized batch of shape `B,C,H,W`).\n",
        "        * `targets` is an iterable of target classes that we want to segment; if None is given, use the top class predicted by the model.\n",
        "        * The result is a numpy array of shape (B, H, W) containing the GradCam heatmap, with `min..max` values rescaled to `0..1` (independently for each image in the batch and each `target_layers`). If more than one `target_layers` was given, return the average of the resulting heatmaps.      \n",
        "    * Feel free to add optional/default arguments and additional methods.\n",
        "* Check your implementation by running the code under the `GradCAM results` header.\n",
        "\n",
        "Tip: you may find it particularly useful to use: [nn.Module.register_full_backward_hook](https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook).\n",
        "\n",
        "\n",
        "## Segmentation task\n",
        "\n",
        "For the second part, SAM is given as a black-box. You must design a pipeline that takes only an image and outputs a segmentation of the circle/square/diamond/triangle/star shape. The input to SAM should be the image and coordinates of point(s) that are likely to be inside (\"foreground\") or likely to be outside (\"background\") of the shape. **The coordinates must be the output of the previous step of the pipeline. They cannot be provided directly by user.**\n",
        "\n",
        "* Think of a way to find appropriate points. Try two different approaches:\n",
        "    1. at least one foreground point, without background points.\n",
        "    2. at least one foreground point and at least one background point.\n",
        "* Implement both approches as subclasses of `BasicSamPipeline`, overriding the `__call__` method (preserving the signature).\n",
        "* Evaluate your generated point(s) and report the following metrics:\n",
        "    * *hit rate*: how often they fall inside the ground-truth mask;\n",
        "    * *distance*: distance from the center of mass of the ground-truth mask\n",
        "        (the average coordinate of True pixels in the mask).\n",
        "* Evaluate your overall pipeline and report the following metric:\n",
        "    * *Intersection over Union (IoU)* of the predicted and ground-truth masks, averaged over all images in the dataset.\n",
        "\n",
        "\n",
        "**Important**: This task is not about finding the pipeline with best hyperparameters; we expect an IoU of at least `65%`, but achieving results higher than that will not affect the grade for the assignment.\n",
        "\n",
        "**Important**: Do not train or fine-tune your own models, only use the ones provided (the classifier and SAM).\n",
        "\n",
        "---\n",
        "\n",
        "## Deliverables\n",
        "\n",
        "On [Moodle](https://moodle.mimuw.edu.pl/mod/assign/view.php?id=159965), submit a .zip archive with:\n",
        "\n",
        "1. **Notebook** (`.ipynb`):\n",
        "    * It should contain:\n",
        "        * The GradCAM implementation.\n",
        "        * The two `BasicSamPipeline` subclasses.\n",
        "        * Evaluations with computations of all reported metrics.\n",
        "    * It should be possible to execute the whole notebook start-to-end without human intervention.\n",
        "    * Such an execution should take less than 30 minutes on a Colab GPU.\n",
        "    * Do not modify (but *do* execute) cells under headers marked as `[do not modify]`. If you wish to extend them (e.g. to check more GradCAM results), you may do so **under a new header**.\n",
        "\n",
        "2. **Report (1-2 pages, PDF)** including:\n",
        "   * An examplary visualization of the output of the Grad-CAM\n",
        "   * A concise description of each approach for the SAM pipelines (1-3 sentences each).\n",
        "   * A presentation of all metrics.\n",
        "   * Discussion (up to 5 sentences) on potential areas for improvements.\n",
        "\n",
        "3. **README.md**:\n",
        "   * Link to Colab version of the notebook for fast replication.\n",
        "\n",
        "\n",
        "\n",
        "## Grading\n",
        "\n",
        "1. Implementation correctness of Grad-CAM:  30%\n",
        "2. Implementation correctness of the multistage pipeline and evaluations: 50%\n",
        "3. Report & analysis: 20%\n",
        "\n",
        "Please take care of readability, clear structure in particular (headers in notebooks, modular code).\n",
        "This will be considered within each grading component.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhWEaao9W2SF"
      },
      "source": [
        "# 0. Preliminaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMwHjowPDYBj"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEZpNKdv1-Fo"
      },
      "outputs": [],
      "source": [
        "# !pip install torch torchvision matplotlib opencv-python-headless numpy segment-anything"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53n0Mpm36Sou"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "from collections.abc import Callable, Iterable\n",
        "from pathlib import Path\n",
        "from typing import Any, Final, Literal, TypedDict\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import scipy.ndimage\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models\n",
        "from torch import Tensor, nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision.transforms import v2\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiAHPuz5W2SG"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXa0LsWEW2SI"
      },
      "source": [
        "## Dataset [do not modify]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayIU_CAawBTZ"
      },
      "source": [
        "CIFAR-10 download takes 170 MiB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdQ_0VKiW2SI"
      },
      "outputs": [],
      "source": [
        "# %%bash --no-raise-error\n",
        "# mkdir -p data/\n",
        "# wget -nc -q -O data/synthetic_shapes.zip https://www.mimuw.edu.pl/~mwrochna/upload/synthetic_shapes.zip\n",
        "# unzip -d data/ data/synthetic_shapes.zip &> /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKr8TlwTW2SI"
      },
      "outputs": [],
      "source": [
        "class SyntheticData[T](Dataset):\n",
        "    \"\"\"A small synthetic segmentation dataset.\n",
        "\n",
        "    It is a sequence dataset of 5000 tuples (image, class, mask), where:\n",
        "    - image: before transformation, an RGB PIL Image.\n",
        "    - class: int 0..4, the label index.\n",
        "    - mask:  numpy array of dtype=bool, shape (H, W), same size as image.\n",
        "    \"\"\"\n",
        "\n",
        "    CLASSES: Final[tuple[str, ...]] = (\"circle\", \"square\", \"triangle\", \"star\", \"diamond\")\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        path: Path,\n",
        "        split: Literal[\"train\", \"val\", \"test\"],\n",
        "        transform: Callable[[PIL.Image.Image], T],\n",
        "    ) -> None:\n",
        "        self.path = path\n",
        "        self.transform = transform\n",
        "        self.split = split\n",
        "\n",
        "        self.class_to_idx = {class_name: i for i, class_name in enumerate(self.CLASSES)}\n",
        "\n",
        "        images = sorted((self.path / \"images\" / self.split).glob(\"*.png\"))\n",
        "        masks = sorted((self.path / \"masks\" / self.split).glob(\"*.png\"))\n",
        "        labels = sorted((self.path / \"labels\" / self.split).glob(\"*.txt\"))\n",
        "\n",
        "        assert images, f\"No images found in {self.path / 'images' / self.split}\"\n",
        "        assert len(images) == len(masks) == len(labels), (\n",
        "            \"Number of images, masks, and labels must be the same\"\n",
        "        )\n",
        "        assert [p.stem for p in images] == [p.stem for p in masks] == [p.stem for p in labels], (\n",
        "            \"Image/mask/label filename mismatch.\"\n",
        "        )\n",
        "        self.image_names = [p.stem for p in images]\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.image_names)\n",
        "\n",
        "    def __getitem__(self, idx) -> tuple[T, int, np.ndarray]:\n",
        "        name = self.image_names[idx]\n",
        "        img_path = self.path / \"images\" / self.split / f\"{name}.png\"\n",
        "        mask_path = self.path / \"masks\" / self.split / f\"{name}.png\"\n",
        "        label_path = self.path / \"labels\" / self.split / f\"{name}.txt\"\n",
        "\n",
        "        img = PIL.Image.open(img_path).convert(\"RGB\")\n",
        "        mask = np.array(PIL.Image.open(mask_path), dtype=bool)\n",
        "        label = self.class_to_idx[label_path.read_text().strip()]\n",
        "\n",
        "        img_transformed: T = self.transform(img)\n",
        "\n",
        "        return img_transformed, label, mask\n",
        "\n",
        "\n",
        "def show_image_row(\n",
        "    image_dict: dict[str, PIL.Image.Image | np.ndarray | Tensor], size: float = 3.0\n",
        ") -> None:\n",
        "    n = len(image_dict)\n",
        "    _, axs = plt.subplots(1, n, figsize=(size * n, size), constrained_layout=True, squeeze=True)\n",
        "    if n == 1:\n",
        "        axs = [axs]\n",
        "    for ax, (title, img) in zip(axs, image_dict.items(), strict=True):\n",
        "        ax.imshow(img)\n",
        "        ax.set_title(title)\n",
        "        ax.axis(\"off\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "u0brJlsbW2SI",
        "outputId": "2901ec84-37f5-462f-cc35-4884e48d0972"
      },
      "outputs": [],
      "source": [
        "dataset_path = Path(\"./data/synthetic_shapes\")\n",
        "\n",
        "\n",
        "def example_from_dataset(idx: int = 3):\n",
        "    for split in (\"train\", \"val\", \"test\"):\n",
        "        dataset = SyntheticData(dataset_path, split=split, transform=lambda x: x)\n",
        "        print(f\"{split} dataset size: {len(dataset)}\")\n",
        "\n",
        "    img, label, mask = dataset[idx]\n",
        "    show_image_row({\"Image\": img, \"Mask\": mask})\n",
        "    print(f\"Label: {label} ({SyntheticData.CLASSES[label]})\")\n",
        "\n",
        "\n",
        "example_from_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZPb1h56W2SI"
      },
      "source": [
        "## Training a classifier [do not modify]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eAbbyg07J_6"
      },
      "outputs": [],
      "source": [
        "def train(\n",
        "    model: nn.Module,\n",
        "    train_loader: torch.utils.data.DataLoader,\n",
        "    val_loader: torch.utils.data.DataLoader,\n",
        "    test_loader: torch.utils.data.DataLoader,\n",
        "    epochs: int = 5,\n",
        "    **optimizer_kwargs: Any,\n",
        ") -> None:\n",
        "    print(f\"ðŸš€ Training CNN for {epochs} epochs...\")\n",
        "    optimizer = optim.AdamW(model.parameters(), **optimizer_kwargs)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        _train_epoch(model, train_loader, optimizer, desc=f\"Epoch {epoch + 1}/{epochs} training  \")\n",
        "        val_metrics = evaluate(model, val_loader, desc=f\"Epoch {epoch + 1}/{epochs} validation\")\n",
        "        print(\n",
        "            f\"Epoch {epoch + 1}/{epochs} â€” \"\n",
        "            + f\"val loss: {val_metrics['loss']:.3f}, val acc: {val_metrics['accuracy']:.1%}\"\n",
        "        )\n",
        "\n",
        "    test_metrics = evaluate(model, test_loader, desc=\"Test Evaluation\")\n",
        "    print(\n",
        "        \"âœ… Model training complete: \"\n",
        "        + f\"Test loss: {test_metrics['loss']:.3f}, test acc: {test_metrics['accuracy']:.1%}\"\n",
        "    )\n",
        "\n",
        "\n",
        "def _train_epoch(\n",
        "    model: nn.Module,\n",
        "    dataloader: torch.utils.data.DataLoader,\n",
        "    optimizer: optim.Optimizer,\n",
        "    desc: str,\n",
        ") -> dict[str, float]:\n",
        "    model.train()\n",
        "    device = next(model.parameters()).device\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    progress_bar = tqdm(dataloader, desc=desc)\n",
        "    for imgs, labels, _ in progress_bar:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs)\n",
        "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, dim=1)\n",
        "        total_samples += labels.shape[0]\n",
        "        total_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        progress_bar.set_postfix(\n",
        "            train_loss=f\"{total_loss / (total_samples / labels.shape[0]):.3f}\",\n",
        "            train_acc=f\"{total_correct / total_samples:.1%}\",\n",
        "        )\n",
        "\n",
        "    return {\"loss\": total_loss / len(dataloader), \"accuracy\": total_correct / total_samples}\n",
        "\n",
        "\n",
        "def evaluate(\n",
        "    model: nn.Module, dataloader: torch.utils.data.DataLoader, desc: str\n",
        ") -> dict[str, float]:\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels, _mask in tqdm(dataloader, desc=desc):\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            outputs = model(imgs)\n",
        "            loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, dim=1)\n",
        "            total_samples += labels.shape[0]\n",
        "            total_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return {\"loss\": total_loss / len(dataloader), \"accuracy\": total_correct / total_samples}\n",
        "\n",
        "\n",
        "class DataloaderArgs(TypedDict, total=False):\n",
        "    batch_size: int\n",
        "    shuffle: bool\n",
        "    num_workers: int\n",
        "    pin_memory: bool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aibqp-L-W2SJ",
        "outputId": "6791862a-03f7-4ef0-ac7a-b7f7ba423f6a"
      },
      "outputs": [],
      "source": [
        "device = torch.accelerator.current_accelerator(check_available=True) or torch.device(\"cpu\")\n",
        "use_accel = device != torch.device(\"cpu\")\n",
        "print(use_accel, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfU07l1CW2SJ",
        "outputId": "f056a181-eb22-48a7-c608-af8d44241d1d"
      },
      "outputs": [],
      "source": [
        "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
        "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
        "transform = v2.Compose(\n",
        "    [\n",
        "        v2.ToImage(),\n",
        "        v2.ToDtype(torch.float32, scale=True),\n",
        "        v2.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
        "    ]\n",
        ")\n",
        "inverse_transform = v2.Compose(\n",
        "    [\n",
        "        v2.Normalize(\n",
        "            [-m / s for m, s in zip(IMAGENET_MEAN, IMAGENET_STD, strict=True)],\n",
        "            [1 / s for s in IMAGENET_STD],\n",
        "        ),\n",
        "        v2.ToPILImage(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "train_dataset = SyntheticData(dataset_path, transform=transform, split=\"train\")\n",
        "val_dataset = SyntheticData(dataset_path, transform=transform, split=\"val\")\n",
        "test_dataset = SyntheticData(dataset_path, transform=transform, split=\"test\")\n",
        "\n",
        "train_kwargs: DataloaderArgs = {\n",
        "    \"batch_size\": 128,\n",
        "    \"num_workers\": 2,\n",
        "    \"shuffle\": True,\n",
        "    \"pin_memory\": use_accel,\n",
        "}\n",
        "val_kwargs: DataloaderArgs = {\"batch_size\": 500, \"num_workers\": 2, \"pin_memory\": use_accel}\n",
        "test_kwargs: DataloaderArgs = val_kwargs\n",
        "\n",
        "train_loader = DataLoader(train_dataset, **train_kwargs)\n",
        "val_loader = DataLoader(val_dataset, **val_kwargs)\n",
        "test_loader = DataLoader(test_dataset, **test_kwargs)\n",
        "\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
        "print(f\"Test dataset size: {len(test_dataset)}\")\n",
        "print(\"âœ… DataLoaders created for train, validation, and test sets.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45_h0YFAd45L"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = Path(\"./model_checkpoint.pth\")\n",
        "\n",
        "model = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "model.fc = nn.Linear(model.fc.in_features, len(SyntheticData.CLASSES))\n",
        "model = model.to(device)\n",
        "\n",
        "if not checkpoint_path.exists():\n",
        "    train(model, train_loader, val_loader, test_loader, epochs=5, lr=2e-3, weight_decay=0.05)\n",
        "    torch.save(model.state_dict(), checkpoint_path)\n",
        "else:\n",
        "    model.load_state_dict(torch.load(checkpoint_path, map_location=device, weights_only=True))\n",
        "    metrics = evaluate(model, test_loader, desc=\"Test Evaluation\")\n",
        "    print()\n",
        "    print(\n",
        "        \"âœ… Model loaded from checkpoint: \"\n",
        "        + f\"Test loss: {metrics['loss']:.3f}, test acc: {metrics['accuracy']:.1%}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dNdj7aZvIaG"
      },
      "source": [
        "# 1. GradCAM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjORc8RAJLuX"
      },
      "source": [
        "## GradCAM implementation (add your code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5SHjz6k6vRf"
      },
      "outputs": [],
      "source": [
        "class GradCAM:\n",
        "    \"\"\"\n",
        "    Pure PyTorch implementation of Grad-CAM.\n",
        "\n",
        "    Usage:\n",
        "        grad_cam = GradCAM(model=model, target_layers=[layer1, layer2])\n",
        "        grayscale_cam = grad_cam(input_tensor, targets=[class_id])\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model: nn.Module, target_layers: Iterable[nn.Module]) -> None:\n",
        "      pass\n",
        "\n",
        "    def __call__(self, input_tensor: Tensor, targets: Iterable[int] | None = None) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Returns: numpy array of shape (B, H, W), values 0..1.\n",
        "        \"\"\"\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFMWhQLgWJV3"
      },
      "source": [
        "## GradCAM results [do not modify]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CiV3MlX1WJV3",
        "outputId": "9f518717-4aa5-4acb-8bbf-faa2f6ec6df4"
      },
      "outputs": [],
      "source": [
        "if type(model).__name__ == \"ResNet\":\n",
        "    target_layers = [model.layer2[-1]]\n",
        "else:\n",
        "    target_layers = [\n",
        "        model.get_submodule(\"features.2.0\"),\n",
        "        model.get_submodule(\"features.3.0\"),\n",
        "        model.get_submodule(\"features.4.0\"),\n",
        "    ]\n",
        "print(f\"Using layers for Grad-CAM: {[type(layer).__name__ for layer in target_layers]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dN5eObXpWJV3"
      },
      "outputs": [],
      "source": [
        "def heatmap_to_rgb_image(\n",
        "    heatmap: np.ndarray, min: float | None = None, max: float | None = None\n",
        ") -> PIL.Image.Image:\n",
        "    \"\"\"\n",
        "    Converts a single-channel heatmap to an RGB pillow image using a colormap.\n",
        "\n",
        "    Args:\n",
        "    - heatmap: shape (H, W), will be normalized by mapping min..max to 0..1.\n",
        "    - min: minimum value for normalization, defaults to heatmap.min().\n",
        "    - max: maximum value for normalization, defaults to heatmap.max()\n",
        "    \"\"\"\n",
        "    heatmap = heatmap.astype(np.float32)\n",
        "    if min is None:\n",
        "        min = heatmap.min()\n",
        "    if max is None:\n",
        "        max = heatmap.max()\n",
        "    heatmap = (heatmap - min) / (max - min + 1e-8)\n",
        "    heatmap_uint8 = (np.clip(heatmap, 0.0, 1.0) * 255).astype(np.uint8)\n",
        "    heatmap_color = cv2.applyColorMap(heatmap_uint8, cv2.COLORMAP_JET)\n",
        "    heatmap_color = cv2.cvtColor(heatmap_color, cv2.COLOR_BGR2RGB)\n",
        "    return PIL.Image.fromarray(heatmap_color)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJRq9-3jWJV3",
        "outputId": "90d0d6bd-0bf8-42a8-b08e-9c3833652783"
      },
      "outputs": [],
      "source": [
        "def example_gradcam():\n",
        "    grad_cam = GradCAM(model=model, target_layers=target_layers)\n",
        "\n",
        "    for test_idx in [3, 10, 42]:\n",
        "        img, label, mask = test_dataset[test_idx]\n",
        "\n",
        "        cam = grad_cam(img.unsqueeze(0).to(device), targets=[label])\n",
        "        heatmap_img = heatmap_to_rgb_image(cam.squeeze(0), 0, 1)\n",
        "\n",
        "        show_image_row(\n",
        "            {\n",
        "                \"Input image\": inverse_transform(img),\n",
        "                \"Grad-CAM heatmap\": heatmap_img,\n",
        "                \"Overlay\": PIL.Image.blend(inverse_transform(img), heatmap_img, alpha=0.3),\n",
        "                \"Ground-truth mask\": mask,\n",
        "            }\n",
        "        )\n",
        "\n",
        "\n",
        "example_gradcam()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcUzgMwbsHdy"
      },
      "source": [
        "# 2. Segment Anything Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjGHh7NlDhEI"
      },
      "source": [
        "\n",
        "## Basic usage [do not modify]\n",
        "The checkpoint takes 360 MB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__5fXL7B0Xz8",
        "outputId": "c084b32b-2e90-45db-eeb9-de8760bbe10b"
      },
      "outputs": [],
      "source": [
        "%pip install segment-anything\n",
        "!wget -nc -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jtgf-gDJsHdy"
      },
      "outputs": [],
      "source": [
        "from segment_anything import SamPredictor, sam_model_registry\n",
        "from segment_anything.utils.transforms import ResizeLongestSide\n",
        "\n",
        "sam_checkpoint_path = Path(\"./sam_vit_b_01ec64.pth\")\n",
        "assert sam_checkpoint_path.exists(), \"SAM checkpoint not found.\"\n",
        "\n",
        "# We'll use a single global SAM model to avoid reloading it to memory multiple times.\n",
        "sam_model = sam_model_registry[\"vit_b\"](checkpoint=sam_checkpoint_path)\n",
        "sam_model.to(device)\n",
        "sam_predictor = SamPredictor(sam_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ju-OgJclsHdy"
      },
      "outputs": [],
      "source": [
        "class BasicSamPipeline:\n",
        "    def __call__(self, images: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Input: normalized images, shape (B, C=3, H, W).\n",
        "        Output: masks tensor of shape (B, H, W), dtype=bool.\n",
        "        \"\"\"\n",
        "        B, C, H, W = images.shape\n",
        "        # The basic pipeline always uses a single center point for each image.\n",
        "        point_coords = np.array([[(W // 2, H // 2)] for _ in range(B)])\n",
        "\n",
        "        # The basic pipeline always uses a single foreground point, no background points.\n",
        "        point_labels = np.array([[1] for _ in range(B)], dtype=np.int64)\n",
        "\n",
        "        return self.segment(images, point_coords, point_labels)\n",
        "\n",
        "    def segment(\n",
        "        self, images: Tensor, point_coords: np.ndarray, point_labels: np.ndarray\n",
        "    ) -> Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "        - images: normalized images, shape (B, C=3, H, W).\n",
        "        - point_coords: point coordinates within each image, shape (B, num_points, 2), format (x,y).\n",
        "            Note the format is not (h,w)=(y,x), but (x,y)!\n",
        "        - point_labels: point labels, shape (B, num_points), dtype int64.\n",
        "            Label 1 is foreground (should be in mask), 0 is background (shouldn't be in mask).\n",
        "\n",
        "        Returns: segmentation masks, shape (B, H, W), dtype=bool.\n",
        "        \"\"\"\n",
        "        B, C, H, W = images.shape\n",
        "        assert C == 3, f\"Expected images.shape=(B, C=3, H, W), got: {images.shape}\"\n",
        "        num_points = point_coords.shape[1]\n",
        "        assert point_coords.shape == (B, num_points, 2), f\"Expected point_coords.shape=({B=}, num_points, 2), got: {point_coords.shape}\"\n",
        "        assert point_labels.shape == (B, num_points), f\"Expected point_labels.shape=({B=}, num_points), got: {point_labels.shape}\"\n",
        "\n",
        "        results = list[Tensor]()\n",
        "        for image, pt_coords, pt_labels in zip(images, point_coords, point_labels, strict=True):\n",
        "            sam_predictor.set_image(np.array(inverse_transform(image)))\n",
        "            masks, scores, _logits = sam_predictor.predict(\n",
        "                point_coords=pt_coords, point_labels=pt_labels, multimask_output=True\n",
        "            )\n",
        "            best_mask = masks[np.argmax(scores)]\n",
        "            results.append(torch.tensor(best_mask, dtype=torch.bool))\n",
        "        return torch.stack(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riC_HistsHdy"
      },
      "outputs": [],
      "source": [
        "def example_sam():\n",
        "    indices = [3, 10, 42]\n",
        "    images, labels, gt_masks = next(iter(test_loader))\n",
        "    images, labels, gt_masks = images[indices], labels[indices], gt_masks[indices]\n",
        "\n",
        "    basic_pipeline = BasicSamPipeline()\n",
        "\n",
        "    results = basic_pipeline(images).cpu()\n",
        "\n",
        "    for image, result, gt_mask in zip(images, results, gt_masks, strict=True):\n",
        "        show_image_row(\n",
        "            {\n",
        "                \"Input image\": inverse_transform(image),\n",
        "                \"Basic SAM predicted mask\": result,\n",
        "                \"Ground-truth mask\": gt_mask,\n",
        "            }\n",
        "        )\n",
        "\n",
        "\n",
        "example_sam()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Js6F1fzJsHdz"
      },
      "source": [
        "## Pipeline implementation and evaluation (add your code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrS-h1gwvyfC"
      },
      "outputs": [],
      "source": [
        "sam_eval_loader = DataLoader(test_dataset, batch_size=5, num_workers=0, pin_memory=use_accel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRwI-rwPwpwv"
      },
      "source": [
        "Add any necessary cells regarding pipeline and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvfLJI5AxmJW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "jupyter2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
